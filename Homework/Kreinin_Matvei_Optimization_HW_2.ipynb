{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a6fcbb6",
      "metadata": {
        "id": "0a6fcbb6"
      },
      "source": [
        "# Домашнее задание 2\n",
        "\n",
        "Это домашнее задание по материалам второго семинаров. Дедлайн по отправке - 23:55 17 февраля. \n",
        "\n",
        "Домашнее задание выполняется в этом же Jupyter Notebook'e и присылается мне на почту: __beznosikov.an@phystech.edu__.\n",
        "\n",
        "Решение каждой задачи необходимо поместить после её условия.\n",
        "\n",
        "Файл должен называться: Фамилия_Имя_Optimization_HW_2\n",
        "\n",
        "При полном запуске Вашего решения (Kernel -> Restart & Run All) все ячейки должны выполняться без ошибок. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1a30ed",
      "metadata": {
        "id": "1b1a30ed"
      },
      "source": [
        "## Задача 1\n",
        "\n",
        "Рассмотрим проекцию на единичном $\\ell_1$-шар:\n",
        "\\begin{equation*}\n",
        "    \\text{proj}_{C}(x) = \\arg\\min_{y \\in C} \\|x-y\\|^2,\n",
        "\\end{equation*}\n",
        "где $C = \\{x ~|~ \\|x\\|_1 \\leq 1 \\}$.\n",
        "\n",
        "__(а)__ Найдите выражение для такой проекции в явном виде."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LY6yR-_A9UCj",
      "metadata": {
        "id": "LY6yR-_A9UCj"
      },
      "source": [
        "Алгоритм проекции:\n",
        "0. Считаем l1 норму вектора x, если меньше или равно. то выдаем x, иначе переходим к шагу.\n",
        "1. Смотрим на координаты вектора x, из них понимаем в какой квадрат d-мерном пространстве мы попали, из этого понимаем уравнение ближайшей гиперплоскости.\n",
        "2. Из уравнения гиперплоскости мы знаем нормаль к ней, можем найти расстояние от точки x до нашей гиперплоскости.\n",
        "3. Нашли расстояние $h = \\frac{|(x-ro, n)|}{|n|}$, где (r0 = ($(-1)^l$, 0, ..., 0), а $l = sign(x_1)$), тогда можем найти в какой точке перпендикуляр из точки x пересекает нашу гиперплоскость.\n",
        "4. Эта точка будет задаваться выражением: $y = x - h \\cdot n$.\n",
        "5. Вычисляем l1 норму вектора y, если он больше единицы, то перебираем оставшиеся d вершин нашего многогранника (гиперплоскости) и выбираем ту, до которой минимальное расстояние.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e95c5e2",
      "metadata": {
        "id": "0e95c5e2"
      },
      "source": [
        "__(б)__ Сравните:\n",
        "\\begin{equation*}\n",
        "    \\text{proj}_C (x) \\quad \\text{и} \\quad \\text{prox}_{\\lambda \\mathbb{I}_C} (x),\n",
        "\\end{equation*}\n",
        "где $\\lambda > 0$, а $\\mathbb{I}_C$ - индикаторная функция множества $C$:\n",
        "$$\n",
        "\\mathbb{I}_C = \n",
        "\\begin{cases}\n",
        "0 ,& x \\in C \\\\\n",
        "+\\infty, &x \\notin C.\n",
        "\\end{cases}\n",
        "$$\n",
        "Являются ли данные операторы эквивалентными для любых $\\lambda > 0$ и выпуклых $C$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "f143476f",
      "metadata": {
        "id": "f143476f"
      },
      "outputs": [],
      "source": [
        "#ответ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "969a8d32",
      "metadata": {
        "id": "969a8d32"
      },
      "source": [
        "## Задача 2\n",
        "\n",
        "Рассмотрим задачу минимизации эмпирического риска:\n",
        "\\begin{equation}\n",
        "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n l (g(w, x_i), y_i),\n",
        "\\end{equation}\n",
        "где $l$ - функция потерь, $g$ - модель, $w$ - параметры модели, $\\{x_i, y_i\\}_{i=1}^n$ - выборка данных из векторов признаков $x_i$ и меток $y_i$.\n",
        "\n",
        "Далее будем рассматривать линейной модель $g(w, x) = w^T x$ и квадратичную функцию потерь $l(z, y) = (z-y)^2$.\n",
        "\n",
        "__(а)__ Запишите задачу минимизации эмпирического риска с линейной моделью  и квадратичной функцией потерь в векторно-матричном виде, используя $X$ - матрицу из векторов $\\{x_i\\}$ и вектор $y$ - из $\\{y_i\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SrFfBUQf2qbJ",
      "metadata": {
        "id": "SrFfBUQf2qbJ"
      },
      "source": [
        "\\begin{equation}\n",
        "\\min_{w \\in \\mathbb{R}^d} ||w^TX - y||_2, X - \\text{ матрица из векторов}, y - \\text{вектор из }\\{y_i\\}.\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecdb57fe",
      "metadata": {
        "id": "ecdb57fe"
      },
      "source": [
        "__(б)__ К заданию приложен датасет _mushrooms_. С помощью следующего кода сформируйте матрицу $X$ и вектор $y$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "AzmEINQDEtBE",
      "metadata": {
        "id": "AzmEINQDEtBE"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_svmlight_file\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.linalg as la\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "561d93a9",
      "metadata": {
        "id": "561d93a9"
      },
      "outputs": [],
      "source": [
        "dataset = \"mushrooms.txt\" \n",
        "#файл должен лежать в той же деректории, что и notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "e3b303d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3b303d4",
        "outputId": "4c439124-a40c-4d33-e965-a8ff1b3c12b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8124, 112) (8124,)\n"
          ]
        }
      ],
      "source": [
        "data = load_svmlight_file(dataset)\n",
        "X, y = data[0].toarray(), data[1]\n",
        "n, d = X.shape\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d466614",
      "metadata": {
        "id": "1d466614"
      },
      "source": [
        "Разделите (лучше случайно) данные (а значит $X$ и $y$) на две части: обучающую и тестовую (примерно в отношении $4$ к $1$). Опишите, как это было сделано."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "77aa2178",
      "metadata": {
        "id": "77aa2178"
      },
      "outputs": [],
      "source": [
        "#ответ\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "ZK6eqeB76YWT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK6eqeB76YWT",
        "outputId": "69fa215e-c64a-4019-a1a0-754eab39f0ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6499, 112) (6499,)\n",
            "(1625, 112) (1625,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b84fa5",
      "metadata": {
        "id": "d3b84fa5"
      },
      "source": [
        "__(в)__ Для обучающей части $X_{train}$, $y_{train}$ оцените константу $L$ задачи обучения/оптимизации. Будет ли задача выпуклой? Сильно выпуклой?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "0b3d1448",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3d1448",
        "outputId": "9a65e289-6af8-4cfd-c0f8-67f0539d928f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimal eigenvalue:  -3.168735668875253e-14\n",
            "Maximal eigenvalue:  20.677110902977507\n"
          ]
        }
      ],
      "source": [
        "#ответ\n",
        "n, k = X_train.shape\n",
        "hessian = 2/n * X_train.T @ X_train\n",
        "wb, vb = np.linalg.eigh(hessian)\n",
        "print(\"Minimal eigenvalue: \", wb[0])\n",
        "print(\"Maximal eigenvalue: \", wb[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KDkNS8Bt7Bpl",
      "metadata": {
        "id": "KDkNS8Bt7Bpl"
      },
      "source": [
        "Понятно, что задача будет выпуклой, т.к. у нас квадрат евклидовой нормы это выпуклая функция, а внутри нормы стоит аффинная функция, поэтому их композиция будет выпуклой функцией.\n",
        "\n",
        "$L = 20.67$\n",
        "\n",
        "Формально функция потерь является сильно выпуклой функцией, но как мы поняли в прошлом задании, $\\mu = 10^{-5}$ это будет плохо обусловленная матрица, а с $\\mu = 10^{-12}$ будет совсем плохо обусловленная матрица, поэтому из теории считаем, что да, но на практике нет."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c8a1056",
      "metadata": {
        "id": "2c8a1056"
      },
      "source": [
        "__(г)__ Используя материалы прошлого ДЗ (по желанию), решите задачу оптимизации/обучите модель машинного обучения с помощью градиентного спуска. Изобразите график сходимости. Какой критерий сходимости будет использовать? Сделайте вывод. Опишите проблемы, с которыми пришлось столкнуться."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "bf6f1b46",
      "metadata": {
        "id": "bf6f1b46"
      },
      "outputs": [],
      "source": [
        "#@title Класс линейной регрессии\n",
        "class MyLinearRegression:\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        self.fit_intercept = fit_intercept\n",
        "    \n",
        "    def __function(self, X, y, w, n):\n",
        "        return 1/n * la.norm(w @ X.T - y, 2) ** 2\n",
        "\n",
        "    def __grad_function(self, X, y, w, n):\n",
        "        \"\"\"\n",
        "        Функция под\n",
        "        \"\"\"\n",
        "        return 1/n * 2 * X.T @ (w @ X.T - y).T \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Функция подбора параметров линейной модели для квадратичной функции потерь.\n",
        "\n",
        "        Input: \n",
        "        - X     : матрица объектов\n",
        "        - y     : вектор ответов\n",
        "\n",
        "        Returns: none.\n",
        "        \"\"\"\n",
        "\n",
        "        n, k = X.shape\n",
        "        wo = np.array([])\n",
        "        X_train = X\n",
        "\n",
        "        # Добавляем ещё столбец для константы\n",
        "        if self.fit_intercept:\n",
        "            X_train = np.hstack((X, np.ones((n, 1))))\n",
        "            w0 = np.random.rand(k+1)\n",
        "        else:\n",
        "            w0 = np.random.rand(k)\n",
        "        \n",
        "\n",
        "        iter = int(10 ** 7)\n",
        "        \n",
        "        hessian = 2 / n * X_train.T @ X_train\n",
        "        wb, vb = np.linalg.eigh(hessian)\n",
        "        \n",
        "        lr_func = lambda X, y, w, n: 1/wb[-1]\n",
        "        \n",
        "        error_criterion = lambda X, y, w, n: np.linalg.norm(self.__grad_function(X, y, w, n), 2)\n",
        "        \n",
        "        for iter_k in range(1000, iter, 1000):\n",
        "            self._w = self.__gradient_descent(self.__function, self.__grad_function,\n",
        "                                            w0, lr_func,  iter, error_criterion, \n",
        "                                            X_train, y)\n",
        "            \n",
        "            error = error_criterion(X, y, self._w, n)\n",
        "            if (error < eps): break\n",
        "\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Функция предсказания по признакам, уже натренированной модели.\n",
        "        Input: \n",
        "        - X : объекты, по которым будем предксазывать\n",
        "\n",
        "        Returns: предсказания на основе весов, ранее обученной линейной модели.\n",
        "        \"\"\"\n",
        "        n, k = X.shape\n",
        "        X_train = X\n",
        "        if self.fit_intercept:\n",
        "            X_train =np.hstack((X, np.ones((n, 1))))\n",
        "\n",
        "        y_pred = self._w @ X_train.T\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Функция получения весов нашей линейной модели.\n",
        "\n",
        "        Input: None.\n",
        "        Returns: Параметры модели.\n",
        "        \"\"\"\n",
        "        return self._w\n",
        "\n",
        "    def __gradient_descent(self, f, grad_f, x0, \n",
        "                         lr, iter, error_criterion, \n",
        "                         X, y):\n",
        "        \"\"\"\n",
        "        Это градиентный спуск.\n",
        "        Он получает на вход целевую функцию, функцию градиента целевой функции, \n",
        "        начальную точку, функцию learning rate, количество итераций и \n",
        "        функцию подсчета ошибки. И применяетметод градиентного спуска.\n",
        "\n",
        "        Inputs:\n",
        "        - f                 : целевая функция, минимум которой мы хотим найти.\n",
        "        - grad_f            : функция градиента целевой функции.\n",
        "        - x0                : начальная точка.\n",
        "        - lr                : функция learning rate.\n",
        "        - iter              : количество итераций.\n",
        "        - error_criterion   : функция подсчета ошибки\n",
        "        - X_train           : множество объектов (матрица фичей)\n",
        "        - y                 : вектор ответов\n",
        "\n",
        "        Returns:\n",
        "        Наилучшую минимальную точку, которую удалось найти.\n",
        "        \"\"\"\n",
        "    \n",
        "        w = x0\n",
        "        eps = 5*10e-5\n",
        "        n, k = X.shape \n",
        "        for k in range(iter):\n",
        "            prev_w = w\n",
        "            w = w - lr(X, y, w, n) * grad_f(X, y, w, n)\n",
        "            \n",
        "            error = error_criterion(X, y, w, n)\n",
        "\n",
        "            if (error < eps):\n",
        "                return w\n",
        "        return w"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7902e5",
      "metadata": {
        "id": "5f7902e5"
      },
      "source": [
        "__(д)__ Как использовать итоговую модель для предсказания? Ответив на вопрос, сделайте предсказания на тестовой выборке. Сравните с реальными метками. Количество правильно угаданных меток есть точность/accuracy вашей модели. Запустите еще раз процесс обучения из пункта г), постройте как меняется точность модели от номера итерации градиентного спуска (измеряйте не каждую итерацию, а, например, каждую 100 или 1000). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "ce3f56f5",
      "metadata": {
        "id": "ce3f56f5"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mw:\\MIPT\\6_sem\\optimization\\Homework\\Kreinin_Matvei_Optimization_HW_2.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m linear_model \u001b[39m=\u001b[39m MyLinearRegression(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m linear_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m end \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start, \u001b[39m5\u001b[39m)\n",
            "\u001b[1;32mw:\\MIPT\\6_sem\\optimization\\Homework\\Kreinin_Matvei_Optimization_HW_2.ipynb Cell 21\u001b[0m in \u001b[0;36mMyLinearRegression.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m error_criterion \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m X, y, w, n: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__grad_function(X, y, w, n), \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m iter_k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m, \u001b[39miter\u001b[39m, \u001b[39m1000\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__gradient_descent(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__function, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__grad_function,\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                                     w0, lr_func,  \u001b[39miter\u001b[39;49m, error_criterion, \n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                                     X_train, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     error \u001b[39m=\u001b[39m error_criterion(X, y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_w, n)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mif\u001b[39;00m (error \u001b[39m<\u001b[39m eps): \u001b[39mbreak\u001b[39;00m\n",
            "\u001b[1;32mw:\\MIPT\\6_sem\\optimization\\Homework\\Kreinin_Matvei_Optimization_HW_2.ipynb Cell 21\u001b[0m in \u001b[0;36mMyLinearRegression.__gradient_descent\u001b[1;34m(self, f, grad_f, x0, lr, iter, error_criterion, X, y)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m prev_w \u001b[39m=\u001b[39m w\n\u001b[0;32m    <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m w \u001b[39m=\u001b[39m w \u001b[39m-\u001b[39m lr(X, y, w, n) \u001b[39m*\u001b[39m grad_f(X, y, w, n)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m error \u001b[39m=\u001b[39m error_criterion(X, y, w, n)\n\u001b[0;32m    <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m (error \u001b[39m<\u001b[39m eps):\n\u001b[0;32m    <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m w\n",
            "\u001b[1;32mw:\\MIPT\\6_sem\\optimization\\Homework\\Kreinin_Matvei_Optimization_HW_2.ipynb Cell 21\u001b[0m in \u001b[0;36mMyLinearRegression.fit.<locals>.<lambda>\u001b[1;34m(X, y, w, n)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m wb, vb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39meigh(hessian)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m lr_func \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m X, y, w, n: \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mwb[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m error_criterion \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m X, y, w, n: np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__grad_function(X, y, w, n), \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m iter_k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m, \u001b[39miter\u001b[39m, \u001b[39m1000\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__gradient_descent(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__function, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__grad_function,\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                                     w0, lr_func,  \u001b[39miter\u001b[39m, error_criterion, \n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                                     X_train, y)\n",
            "\u001b[1;32mw:\\MIPT\\6_sem\\optimization\\Homework\\Kreinin_Matvei_Optimization_HW_2.ipynb Cell 21\u001b[0m in \u001b[0;36mMyLinearRegression.__grad_function\u001b[1;34m(self, X, y, w, n)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__grad_function\u001b[39m(\u001b[39mself\u001b[39m, X, y, w, n):\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    Функция под\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/w%3A/MIPT/6_sem/optimization/Homework/Kreinin_Matvei_Optimization_HW_2.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49mn \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m X\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39m (w \u001b[39m@\u001b[39m X\u001b[39m.\u001b[39mT \u001b[39m-\u001b[39m y)\u001b[39m.\u001b[39mT\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "linear_model = MyLinearRegression(True)\n",
        "linear_model.fit(X_train, y_train)\n",
        "end = round(time.time() - start, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g_K1aqDDyYMb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_K1aqDDyYMb",
        "outputId": "c79a6b36-e826-4f82-f060-0c7d910da998"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CbU4eGhBC6-W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbU4eGhBC6-W",
        "outputId": "97f590f1-7bde-4314-c928-a6777874bab6"
      },
      "outputs": [],
      "source": [
        "y_pred_sklearn = model.predict(X_test)\n",
        "y_pred_my = linear_model.predict(X_test)\n",
        "y_pred_sklearn = np.round(y_pred_sklearn)\n",
        "y_pred_my = np.round(y_pred_my)\n",
        "\n",
        "\n",
        "r2_score_sklearn = r2_score(y_test, y_pred_sklearn)\n",
        "r2_score_my = r2_score(y_test, y_pred_my)\n",
        "\n",
        "mean_squared_error_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
        "mean_squared_error_my = mean_squared_error(y_test, y_pred_my)\n",
        "\n",
        "accuracy_score_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "accuracy_score_my = accuracy_score(y_test, y_pred_my)\n",
        "\n",
        "print(\"               : sklearn | my regresion\")\n",
        "print(\"R2 score       : %0.5f | %0.5f  \" % (r2_score_sklearn, r2_score_my))\n",
        "print(\"mean_scored    : %0.5f | %0.5f  \" % (mean_squared_error_sklearn, mean_squared_error_my))\n",
        "print(\"accuracy_score : %0.5f | %0.5f  \" % (accuracy_score_sklearn, accuracy_score_my))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29c04712",
      "metadata": {
        "id": "29c04712"
      },
      "source": [
        "__(e)__ Теперь рассмотрим эту же задачу обучения на $\\ell_1$-шаре с радиусом $\\lambda$. Повторите пункты г) и д) только теперь градиентный спуск должен быть модифицирован с помощью оператора проекции. Пробуйте различные $\\lambda > 0$ и найдите лучшее (опишите почему именно такое лучшее). Удалось ли улучшить процесс обучения/точность модели на тестовой выборке? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "46b1db1d",
      "metadata": {
        "id": "46b1db1d"
      },
      "outputs": [],
      "source": [
        "class MyLinearRegressionProj:\n",
        "    def __init__(self, fit_intercept=True):\n",
        "        self.fit_intercept = fit_intercept\n",
        "    \n",
        "    def __function(self, X, y, w, n):\n",
        "        return 1/n * la.norm(w @ X.T - y, 2) ** 2\n",
        "\n",
        "    def __grad_function(self, X, y, w, n):\n",
        "        \"\"\"\n",
        "        Функция градиента функции.\n",
        "\n",
        "        Input:\n",
        "        - X         : матрица фичей\n",
        "        - y         : вектор ответов\n",
        "        - w         : вектор весов модели\n",
        "        - n         : количество экспериментов в матрице\n",
        "        \"\"\"\n",
        "        return 1/n * 2 * X.T @ (w @ X.T - y).T \n",
        "        \n",
        "\n",
        "    def __lr_func(self, k, w, y, X, f, grad_f, n):        \n",
        "        return 1 / self.Lr\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Функция подбора параметров линейной модели для квадратичной функции потерь.\n",
        "\n",
        "        Input: \n",
        "        - X     : матрица объектов\n",
        "        - y     : вектор ответов\n",
        "\n",
        "        Returns: none.\n",
        "        \"\"\"\n",
        "\n",
        "        n, k = X.shape\n",
        "        wo = np.array([])\n",
        "        X_train = X\n",
        "\n",
        "        if self.fit_intercept:\n",
        "            X_train = np.hstack((X, np.ones((n, 1))))\n",
        "            w0 = np.random.rand(k+1)\n",
        "        else:\n",
        "            w0 = np.random.rand(k)\n",
        "        \n",
        "        iter = int(10 ** 7)\n",
        "        hessian = 2 / n * X_train.T @ X_train\n",
        "        wb, vb = np.linalg.eigh(hessian)\n",
        "        \n",
        "        self.Lr = wb[-1] + wb[0]\n",
        "        \n",
        "        #error_criterion = lambda X, y, w, n : np.linalg.norm(self.__grad_function(X, y, w, n), 2) ** 2\n",
        "        error_criterion = lambda X, y, w, n, w_prev : np.linalg.norm(w - w_prev)\n",
        "        self._w = self.__gradient_descent(self.__function, self.__grad_function,\n",
        "                                        w0, self.__lr_func,  iter, \n",
        "                                        error_criterion, X_train, y)\n",
        "\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Функция предсказания по признакам, уже натренированной модели.\n",
        "        Input: \n",
        "        - X : объекты, по которым будем предксазывать\n",
        "\n",
        "        Returns: предсказания на основе весов, ранее обученной линейной модели.\n",
        "        \"\"\"\n",
        "        n, k = X.shape\n",
        "        X_train = X\n",
        "        if self.fit_intercept:\n",
        "            X_train =np.hstack((X, np.ones((n, 1))))\n",
        "\n",
        "        y_pred = self._w @ X_train.T\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Функция получения весов нашей линейной модели.\n",
        "\n",
        "        Input: None.\n",
        "        Returns: Параметры модели.\n",
        "        \"\"\"\n",
        "\n",
        "        return self._w\n",
        "\n",
        "    def proj(self, vector_x):\n",
        "        if (la.norm(vector_x, 1) <= 1):\n",
        "            return vector_x\n",
        "        \n",
        "        vector_n = np.sign(vector_x)\n",
        "        vector_n = vector_n / la.norm(vector_n, 2)\n",
        "\n",
        "        #print(\"Vector normal:\", vector_n)\n",
        "\n",
        "        vector_r0 = np.zeros(len(vector_x))\n",
        "        \n",
        "        for i in range(len(vector_x)):\n",
        "            if vector_x[i] != 0:\n",
        "                vector_r0[i] = np.sign(vector_x[i])\n",
        "                break\n",
        "        #print(\"Vector R0:\", vector_r0)\n",
        "\n",
        "        h = np.abs( (vector_x - vector_r0) @ vector_n) / la.norm(vector_n, 2)\n",
        "        #print(\"Distance to plain: \", h)\n",
        "        \n",
        "        vector_y = vector_x - h * vector_n\n",
        "        eps = 10e-6\n",
        "\n",
        "        #print(\"Vector_y (1): \", vector_y, \" norm_1 = \", la.norm(vector_y, 1))\n",
        "        if (np.abs(la.norm(vector_y, 1) - 1) < eps):\n",
        "            return vector_y\n",
        "        else:\n",
        "            vector_y = vector_x + h * vector_n\n",
        "            #print(\"Vector_y (2): \", vector_y, \" norm_1 = \", la.norm(vector_y, 1))\n",
        "            if (np.abs(la.norm(vector_y, 1) - 1) < eps):\n",
        "                return vector_y\n",
        "        \n",
        "        vector_vert = np.zeros(len(vector_x))\n",
        "        vector_vert[0] = np.sign(vector_x[0])\n",
        "        \n",
        "        min_dist = la.norm(vector_x - vector_vert, 2)\n",
        "        min_vert = 0\n",
        "\n",
        "        for i in range(1, len(vector_x) - 1, 1):\n",
        "            vector_vert[i - 1] = 0\n",
        "            vector_vert[i] = np.sign(vector_x[i])\n",
        "            dist_i = la.norm(vector_x - vector_vert, 2)\n",
        "            if (dist_i < min_dist):\n",
        "                min_vert = i\n",
        "                min_dist = dist_i\n",
        "        \n",
        "        vector_proj = np.zeros(len(vector_x))\n",
        "        vector_proj[min_vert] = np.sign(vector_x[min_vert])\n",
        "        \n",
        "        return vector_proj\n",
        "\n",
        "\n",
        "    def __gradient_descent(self, f, grad_f, x0, \n",
        "                         lr, iter, error_criterion, \n",
        "                         X, y):\n",
        "        \"\"\"\n",
        "        Это градиентный спуск.\n",
        "        Он получает на вход целевую функцию, функцию градиента целевой функции, \n",
        "        начальную точку, функцию learning rate, количество итераций и \n",
        "        функцию подсчета ошибки. И применяетметод градиентного спуска.\n",
        "\n",
        "        Inputs:\n",
        "        - f                 : целевая функция, минимум которой мы хотим найти.\n",
        "        - grad_f            : функция градиента целевой функции.\n",
        "        - x0                : начальная точка.\n",
        "        - lr                : функция learning rate.\n",
        "        - iter              : количество итераций.\n",
        "        - error_criterion   : функция подсчета ошибки\n",
        "        - X_train           : множество объектов (матрица фичей)\n",
        "        - y                 : вектор ответов\n",
        "\n",
        "        Returns:\n",
        "        Наилучшую минимальную точку, которую удалось найти.\n",
        "        \"\"\"\n",
        "    \n",
        "        w = x0\n",
        "        eps = 10e-7\n",
        "        n, k = X.shape \n",
        "        for k in range(iter):\n",
        "            prev_w = w\n",
        "            w = w - lr(k, w, y, X, f, grad_f, n) * self.proj(grad_f(X, y, w, n))\n",
        "            \n",
        "            error = error_criterion(X, y, w, n, prev_w)\n",
        "\n",
        "            if (error < eps):\n",
        "                return w\n",
        "            if (k % 1000 == 0):\n",
        "                print(\"%d - %f\" % (k, error))\n",
        "        print(\"Final number of operations: %d\" % (k))\n",
        "        return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "02c9c04d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 - 0.044123\n",
            "1000 - 0.001184\n",
            "2000 - 0.000478\n",
            "3000 - 0.000268\n",
            "4000 - 0.000171\n",
            "5000 - 0.000119\n",
            "6000 - 0.000089\n",
            "7000 - 0.000071\n",
            "8000 - 0.000060\n",
            "9000 - 0.000052\n",
            "10000 - 0.000045\n",
            "11000 - 0.000040\n",
            "12000 - 0.000036\n",
            "13000 - 0.000033\n",
            "14000 - 0.000030\n",
            "15000 - 0.000027\n",
            "16000 - 0.000025\n",
            "17000 - 0.000023\n",
            "18000 - 0.000021\n",
            "19000 - 0.000020\n",
            "20000 - 0.000018\n",
            "21000 - 0.000017\n",
            "22000 - 0.000016\n",
            "23000 - 0.000015\n",
            "24000 - 0.000014\n",
            "25000 - 0.000014\n",
            "26000 - 0.000013\n",
            "27000 - 0.000012\n",
            "28000 - 0.000011\n",
            "29000 - 0.000011\n",
            "30000 - 0.000010\n",
            "31000 - 0.000010\n",
            "32000 - 0.000009\n",
            "33000 - 0.000009\n",
            "34000 - 0.000009\n",
            "35000 - 0.000008\n",
            "36000 - 0.000008\n",
            "37000 - 0.000008\n",
            "38000 - 0.000007\n",
            "39000 - 0.000007\n",
            "40000 - 0.000007\n",
            "41000 - 0.000007\n",
            "42000 - 0.000006\n",
            "43000 - 0.000006\n",
            "44000 - 0.000006\n",
            "45000 - 0.000006\n",
            "46000 - 0.000006\n",
            "47000 - 0.000005\n",
            "48000 - 0.000005\n",
            "49000 - 0.000005\n",
            "50000 - 0.000005\n",
            "51000 - 0.000005\n",
            "52000 - 0.000005\n",
            "53000 - 0.000005\n",
            "54000 - 0.000004\n",
            "55000 - 0.000004\n",
            "56000 - 0.000004\n",
            "57000 - 0.000004\n",
            "58000 - 0.000004\n",
            "59000 - 0.000004\n",
            "60000 - 0.000004\n",
            "61000 - 0.000004\n",
            "62000 - 0.000004\n",
            "63000 - 0.000004\n",
            "64000 - 0.000003\n",
            "65000 - 0.000003\n",
            "66000 - 0.000003\n",
            "67000 - 0.000003\n",
            "68000 - 0.000003\n",
            "69000 - 0.000003\n",
            "70000 - 0.000003\n",
            "71000 - 0.000003\n",
            "72000 - 0.000003\n",
            "73000 - 0.000003\n",
            "74000 - 0.000003\n",
            "75000 - 0.000003\n",
            "76000 - 0.000003\n",
            "77000 - 0.000003\n",
            "78000 - 0.000003\n",
            "79000 - 0.000003\n",
            "80000 - 0.000002\n",
            "81000 - 0.000002\n",
            "82000 - 0.000002\n",
            "83000 - 0.000002\n",
            "84000 - 0.000002\n",
            "85000 - 0.000002\n",
            "86000 - 0.000002\n",
            "87000 - 0.000002\n",
            "88000 - 0.000002\n",
            "89000 - 0.000002\n",
            "90000 - 0.000002\n",
            "91000 - 0.000002\n",
            "92000 - 0.000002\n",
            "93000 - 0.000002\n",
            "94000 - 0.000002\n",
            "95000 - 0.000002\n",
            "96000 - 0.000002\n",
            "97000 - 0.000002\n",
            "98000 - 0.000002\n",
            "99000 - 0.000002\n",
            "100000 - 0.000002\n",
            "101000 - 0.000002\n",
            "102000 - 0.000002\n",
            "103000 - 0.000002\n",
            "104000 - 0.000002\n",
            "105000 - 0.000002\n",
            "106000 - 0.000002\n",
            "107000 - 0.000002\n",
            "108000 - 0.000001\n",
            "109000 - 0.000001\n",
            "110000 - 0.000001\n",
            "111000 - 0.000001\n",
            "112000 - 0.000001\n",
            "113000 - 0.000001\n",
            "114000 - 0.000001\n",
            "115000 - 0.000001\n",
            "116000 - 0.000001\n",
            "117000 - 0.000001\n",
            "118000 - 0.000001\n",
            "119000 - 0.000001\n",
            "120000 - 0.000001\n",
            "121000 - 0.000001\n",
            "122000 - 0.000001\n",
            "123000 - 0.000001\n",
            "124000 - 0.000001\n",
            "125000 - 0.000001\n",
            "126000 - 0.000001\n",
            "127000 - 0.000001\n",
            "128000 - 0.000001\n",
            "129000 - 0.000001\n",
            "130000 - 0.000001\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "linear_model = MyLinearRegressionProj(True)\n",
        "linear_model.fit(X_train, y_train)\n",
        "end = round(time.time() - start, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "8610a31f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "e201b1ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               : sklearn | my regresion\n",
            "R2 score       : 1.00000 | 0.99753  \n",
            "mean_scored    : 0.00000 | 0.00062  \n",
            "accuracy_score : 1.00000 | 0.99938  \n"
          ]
        }
      ],
      "source": [
        "y_pred_sklearn = model.predict(X_test)\n",
        "y_pred_my = linear_model.predict(X_test)\n",
        "y_pred_sklearn = np.round(y_pred_sklearn)\n",
        "y_pred_my = np.round(y_pred_my)\n",
        "\n",
        "\n",
        "r2_score_sklearn = r2_score(y_test, y_pred_sklearn)\n",
        "r2_score_my = r2_score(y_test, y_pred_my)\n",
        "\n",
        "mean_squared_error_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
        "mean_squared_error_my = mean_squared_error(y_test, y_pred_my)\n",
        "\n",
        "accuracy_score_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "accuracy_score_my = accuracy_score(y_test, y_pred_my)\n",
        "\n",
        "print(\"               : sklearn | my regresion\")\n",
        "print(\"R2 score       : %0.5f | %0.5f  \" % (r2_score_sklearn, r2_score_my))\n",
        "print(\"mean_scored    : %0.5f | %0.5f  \" % (mean_squared_error_sklearn, mean_squared_error_my))\n",
        "print(\"accuracy_score : %0.5f | %0.5f  \" % (accuracy_score_sklearn, accuracy_score_my))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56317873",
      "metadata": {
        "id": "56317873"
      },
      "source": [
        "__Бонусные пункт__\n",
        "\n",
        "__(ж)__ Снова рассмотрим задачу обучения без ограничений, но добавим к ней регуляризатор 1) $\\lambda \\| x \\|_1$ c $\\lambda  > 0$, 2) $\\lambda \\| x \\|_2$ c $\\lambda  > 0$. Сможем ли для новой задачи с регуляризатором использовать честный градиентный спуск без модификаций? Почему? Предложите свои идеи, как модифицировать градиентный спуск для новой задачи. Проведите для него эксперименты из пунктов г) и д).  Пробуйте различные $\\lambda > 0$ и найдите лучшее (опишите почему именно такое лучшее). Сравните результаты с пунктом e)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caad1f26",
      "metadata": {
        "id": "caad1f26"
      },
      "outputs": [],
      "source": [
        "#ответ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5de0ffdd",
      "metadata": {
        "id": "5de0ffdd"
      },
      "source": [
        "__(з)__ Воспользуемся для нашей задачи методом [PCA](https://ru.wikipedia.org/wiki/Метод_главных_компонент). Прочитайте про него подробнее и опишите основную идею."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1b1c66",
      "metadata": {
        "id": "db1b1c66"
      },
      "outputs": [],
      "source": [
        "#ответ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d55f20",
      "metadata": {
        "id": "35d55f20"
      },
      "source": [
        "Следующий код запускает PCA для нашего датасета. Убедитесь, что понимаете происходящие."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a91111",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3a91111",
        "outputId": "e7d8538b-0d74-4012-c17b-fa5a7b636b39"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=10)\n",
        "pca.fit(X_train)  #эта строчка будет работать только если есть матрица X_train\n",
        "print (pca.explained_variance_ratio_.cumsum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086cf178",
      "metadata": {
        "id": "086cf178"
      },
      "source": [
        "Какое значение n_components разумно поставить? Для новой матрицы X_train повторите пункт ж). Измерьте время работы процесса обучения. Постройте график точности решения на тестовой выборке от времени работы для честного X_train и полученного с помощью PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc380da1",
      "metadata": {
        "id": "bc380da1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "52b1068d7c9357a6bcf4bebbc496f97beb29e7d49e66fdfa07b160af1e74d97d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
